{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project   ID:3034093"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the classes in the last week , Professor mentioned that the fundamental deep learning called \"Gradient descent.\" Thus, I think that I could do some research for this optimizers. With the Internet, I was surprised by so many different optimizers come from \"Gradient descent.\" In my final project, I will talk about another three commons optimizers' properties and use a very simple code to show the different effects in the data.\n",
    "\n",
    "I will use simple tensorflow code refer from tensorflow official website to show how different effects for different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = np.linspace(-1,1,300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(2*x_data) - 0.5 + noise\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, 1])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "\n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),reduction_indices=[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function.\n",
    "With grsdient descent's properties, we can really easy to find the best weight in our data,that is why we choose gradient decent as our fundamental deeping learning's optimizers. For mathematical thinking:\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{k} = \\mathbf{w}^{k-1} − α\\frac{∇g(wk−1)}{∥∇g(wk−1)∥_2}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.91731\n",
      "1.29152\n",
      "1.16007\n",
      "1.08471\n",
      "1.00149\n",
      "0.90754\n",
      "0.804457\n",
      "0.696484\n",
      "0.589259\n",
      "0.488354\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(10**(-1)).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(50):\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 5 == 0:\n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loss history when we use gradient descent as optmizer is down for a little bit slow. That is why we try to use some ways to optimize our gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Momentum is one of the methods which could help us get better result from slightly append something to gradient descent.We can imagine that Momentum is a method we put our gradient descent on the slope, then if it starts to go down, it will be faster and straighter down to the minimum.\n",
    "For mathematical thinking:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Momentum} = {β}*\\mathbf{Momentum} − α\\frac{∇g(wk−1)}{∥∇g(wk−1)∥_2}\n",
    "\\end{equation}\n",
    " \n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{k} = \\mathbf{w}^{k-1} +\\mathbf{Momentum}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.56091\n",
      "0.727871\n",
      "0.147081\n",
      "0.0629346\n",
      "0.0410063\n",
      "0.0215965\n",
      "0.0163123\n",
      "0.0131879\n",
      "0.0116999\n",
      "0.010458\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.MomentumOptimizer(0.1,0.8).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(50):\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 5 == 0:\n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result, we could find that our loss history goes faster and faster down to the hill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Adaptive Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, Adagrad is another way to help us have a better and faster result in gradient descent. The basic idea for adagrad is  let learning rate to adapt to the parameters, giving different updates for different frequency parameters. I would like to explain adagrad as a pair of shoes which are worn by gradient descent. Once gradient descent wants to detour, adagrad will hurt it legs to make sure gradient descent only can go straight down to the minimum. For mathematical thinking:\n",
    "\\begin{equation}\n",
    "\\mathbf{v}^{k} = \\mathbf{v}^{k-1} + {∥\\frac{∇g(wk−1)}{∥∇g(wk−1)∥_2}∥}^2\n",
    "\\end{equation}\n",
    " \n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{k} = \\mathbf{w}^{k-1} − α\\frac{\\frac{∇g(wk−1)}{∥∇g(wk−1)∥_2}}{\\sqrt{v}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12716\n",
      "0.631589\n",
      "0.35323\n",
      "0.243232\n",
      "0.181441\n",
      "0.143103\n",
      "0.11831\n",
      "0.101847\n",
      "0.0907083\n",
      "0.0829642\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdagradOptimizer(0.1,0.2).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(50):\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 5 == 0:\n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss history result from Adagrad obviously better than gradient descent but we can also find that seems worse than momentum. Honestly, it all depends on parameters which set by ourselves. Thus, we just have to compare with gradient descent but not others optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Moment Estimation Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, Adam, an algorithm which combines momentum and adagrad. Adam can solve the problem of Adagrad's rapid decline in learning rates, so it also can be regarded as an optimization for Adagrad. Now, gradient descent wears a pair of shoes and faces a slope, having both advantages from Momentum and Adagrad, we should find the best result in loss history. For mathematical thinking:\n",
    "\\begin{equation}\n",
    "\\mathbf{Momentum} = {β_1}\\mathbf{Momentum} + (1 - {β_1})  \\frac{∇g(wk−1)}{∥∇g(wk−1)∥_2}\n",
    "\\end{equation}\n",
    " \n",
    "\\begin{equation} \n",
    "\\mathbf{v} = {β_2}\\mathbf{v} +(1 - {β_2}) {∥\\frac{∇g(wk−1)}{∥∇g(wk−1)∥_2}∥}^2\n",
    "\\end{equation}\n",
    " \n",
    "\\begin{equation} \n",
    "\\mathbf{w}^{k} = \\mathbf{w}^{k-1} − \\frac{α\\mathbf{Momentum}}{\\sqrt{v}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58933\n",
      "0.431327\n",
      "0.144372\n",
      "0.138544\n",
      "0.0786647\n",
      "0.0483788\n",
      "0.032161\n",
      "0.0170096\n",
      "0.0188005\n",
      "0.0140134\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.1,0.9,0.999).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(50):\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 5 == 0:\n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our expectation, Adam got a faster and smaller result in the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these others optimizers, we can conclude that gradient descent told us how to go down to the minimum but did not have any restrictions. Thus, Momentum and Adagrad are two distinct types to go down to the minimum. They have their own limits and rules which could faster and straighter go to the minimum. However, Adam is an algorithm which has these two method's advantages. That is why we can mostly get an ideal loss history by using Adam as our optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization(John Duchi, Elad Hazan, Yoram Singer, 2011)\n",
    " \n",
    "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION(Diederik P. Kingma, Jimmy Lei Ba, 2015)\n",
    "https://jermwatt.github.io/mlrefined/blog_posts/Mathematical_Optimization/Part_2_gradient_descent.html\n",
    " \n",
    "https://github.com/MorvanZhou\n",
    " \n",
    "https://www.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
